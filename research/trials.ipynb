{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46946c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ASUS\\\\Downloads\\\\MediGenie\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38711f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975b2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394fb884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ASUS\\\\Downloads\\\\MediGenie'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5815c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader  #yha pr hmne pdf file use ki h , aur directory ke andr\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #for chunking operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb1da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data From the PDF File\n",
    "\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader ) #it will extend the info from pdf\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b284429",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data=r\"C:\\Users\\ASUS\\Downloads\\MediGenie\\Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06deac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data                    # (we may see our data by running it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2900c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to perform the chunking operation, we use RecursiveCharacterTextSplitter\n",
    "# Split the Data into Text Chunks\n",
    "\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9260fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text Chunks: 5859\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of text Chunks:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878b4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f22966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30cc7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embeddings from HuggingFace\n",
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a4dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\.conda\\envs\\medigenie-py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2316335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19448\\717155546.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "embeddings= download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1de3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "# now test the model whether it is able to convert my sentence into vector embedding and check ki\n",
    "# ye hmara 384 vector dimension dera h ya ni (which we het from hugging face transfromer link?\n",
    "\n",
    "query_result=embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\",len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6caa363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result                 #to yha pr ye hme complete vector show krega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1667f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now initialize the pinecon \n",
    "\n",
    "# on pinecone.ai hm apna cluster create kreinge to save all vectors and to create API Key \n",
    "# see it in .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd922938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hm apna index manually bhi create kr skte h , but its time consuming so we load from data\n",
    "# \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb886ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have your Pinecone API key set as an environment variable\n",
    "import os\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5f43a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'medicalbot' created.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Define your index name\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "# Check if it already exists\n",
    "existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "if index_name not in existing_indexes:\n",
    "    # Only create if it doesn't exist\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,   #yha pr hme apne hugging model ka dimension size dena h\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(f\"Index '{index_name}' created.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists â€” skipping creation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d869a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we may convert our chunks into vector embedding and store them in Pinecone\n",
    "\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2078e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Set your Gemini API Key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b139e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings upserted into Pinecone index successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Assuming you have 'text_chunks', 'index_name', and 'embeddings' defined\n",
    "# Embed each chunk and upsert the embeddings into your pinecone index to the pinecone vector database\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "print(\"Embeddings upserted into Pinecone index successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd71e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yha hm notice kreinge ki hmaare starting me len(text_chunks)ka size tha pinecone vector db me store krne\n",
    "# pr bhi same record count h\n",
    "\n",
    "#and here it acts as our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95ef5b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded existing Pinecone index 'medicalbot'.\n"
     ]
    }
   ],
   "source": [
    "# Load Existing Index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Embed each chunk and upsert the embeddings into your pinecone index\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(   #it will load all index in docsearch\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded existing Pinecone index '{index_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "054891a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x2666b7136d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3897bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  #here it give 3 line responses\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2189ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='9ac59ecc-be7b-4cef-a89d-3f753a1b8bf5', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'C:\\\\Users\\\\ASUS\\\\Downloads\\\\MediGenie\\\\Data\\\\Medical_book.pdf.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='4857d146-586d-4853-a5a3-ffb3df44a011', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'C:\\\\Users\\\\ASUS\\\\Downloads\\\\MediGenie\\\\Data\\\\Medical_book.pdf.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
       " Document(id='02993d88-e5eb-4542-bf8c-712fd6b10684', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 37.0, 'page_label': '38', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'C:\\\\Users\\\\ASUS\\\\Downloads\\\\MediGenie\\\\Data\\\\Medical_book.pdf.pdf', 'total_pages': 637.0}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs  # yha pr hm chahte h ki hme aise ni proper definition ki form me result mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c87154da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I want the proper definition instead of this type of output, so we insert the LLM Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9571eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY   # Replace with your actual API key\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06be8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getenv(\"OPENAI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9b721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",   # or \"gemini-pro\"\n",
    "    temperature=0,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c66066f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (                                  #here i will give this prompt to my LLM\n",
    "    \"You are an assistant for question-answering tasks.\\n\"\n",
    "    \"Use the following pieces of retrieved context to answer\"\n",
    "    \"the question. If you don't know the answer, say that you\"\n",
    "    \"don't know. Use three sentences maximum and keep the \" \n",
    "    \"answer concise.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(    # give complete output to the user input query\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bb7b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "\n",
    "# model = genai.GenerativeModel('gemini-pro')\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "\n",
    "response = model.generate_content(\"What is acne?\")\n",
    "# print(response.text)\n",
    "\n",
    "# print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e126afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets create our question answer chain\n",
    " \n",
    "question_answer_chain=create_stuff_documents_chain(llm,prompt)\n",
    "rag_chain= create_retrieval_chain(retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68fdb6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acne is a common skin disease characterized by pimples on the face, chest, and back.  It happens when skin pores become clogged with oil, dead skin cells, and bacteria.  Acne vulgaris is the medical term for common acne.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# response = rag_chain.invoke({\"input\": \"What is Stats?\"})        #it will generate I dont know bcoz\n",
    "# print(response[\"answer\"])                                     #it is outside to our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3fed376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_google_genai.chat_models.ChatGoogleGenerativeAI'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db26e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4209b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medigenie-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
